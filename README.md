# 


torchrun --master-port 29520 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 8


torchrun --master-port 29530 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 9


torchrun --master-port 29540 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 10

torchrun --master-port 29550 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 11

torchrun --master-port 29560 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 12

torchrun --master-port 29570 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 13



torchrun --master-port 29520 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset truthfulqa --k 0

torchrun --master-port 29530 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset truthfulqa --k 1

torchrun --master-port 29540 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset truthfulqa --k 2



torchrun --master-port 29550 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 0

torchrun --master-port 29560 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 1

torchrun --master-port 29570 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 2

torchrun --master-port 29580 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 3

torchrun --master-port 29590 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 4



torchrun --master-port 29600 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 5

torchrun --master-port 29560 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 6



torchrun --master-port 29570 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 7

torchrun --master-port 29581 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 8


torchrun --master-port 29582 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 9


torchrun --master-port 29583 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 10; torchrun --master-port 29583 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 11


torchrun --master-port 29584 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 12; torchrun --master-port 29584 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 13

torchrun --master-port 29585 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 14; torchrun --master-port 29585 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 15


torchrun --master-port 29586 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 16; torchrun --master-port 29586 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 17


torchrun --master-port 29587 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 18; torchrun --master-port 29587 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 19


torchrun --master-port 29588 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 20; torchrun --master-port 29588 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset hotpotqa --k 21



torchrun --master-port 29520 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset truthfulqa --k 25

torchrun --master-port 29530 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset truthfulqa --k 26

torchrun --master-port 29540 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset truthfulqa --k 27

torchrun --master-port 29541 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset truthfulqa --k 28

torchrun --master-port 29542 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset truthfulqa --k 29

torchrun --master-port 29543 /home/qblocks/reflective_thinking/run_llama.py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model  --max_seq_len 4096 --max_batch_size 6 --dataset truthfulqa --k 30